JSON Structure:
Below json has array type as structure in sensor readings.
1. Read sensor readings as array
2. Explode it to multiple columns


{
    "sensorName": "snx001",
    "sensorDate": "2020-01-01",
    "sensorReadings": [
        {
            "sensorChannel": 1,
            "sensorReading": 3.7465084060850105,
            "datetime": "2020-01-01 00:00:00"
        },
        {
            "sensorChannel": 2,
            "sensorReading": 10.543041369293153,
            "datetime": "2020-01-01 00:00:00"
        }
        ...
    ]
}


# Defining JSON schema using StructType

schema = StructType([
  StructField("sensorName", StringType()),
  StructField("sensorDate", DateType()),
  StructField("sensorReadings", ArrayType([
    StructType([
      StructField("sensorChannel", IntegerType()),
      StructField("sensorReading", DoubleType()),
      StructField("datetime", TimestampType())
    ])
  ]))
])

# Read json file with multiline true

sensor_df = spark.read.format("json").schema(schema).option("multiline",True).option("path", "").load()

sensor_df.select(
  "sensorName",
  "sensorDate",
  explode("sensorReadings").alias("sensorReadingsExploded")
).select(
  "sensorName",
  "sensorDate",
  "sensorReadingsExploded.*"
)

>>> Exploding above so that we can have a record for each sensorReading



from pyspark.sql import SparkSession
from pyspark import SparkConf

from pyspark.sql.types import StructType, StructField, ArrayType, StringType, DateType, IntegerType, DoubleType, TimestampType
from pyspark.sql import functions as F

spark_config = SparkConf()
spark_config.set("spark.app.name", "JSON Reader")
spark_config.set("spark.master", "local[*]")

spark = SparkSession.builder\
    .config(conf=spark_config)\
    .getOrCreate()

sensor_schema = StructType([
    StructField("sensorName", StringType()),
    StructField("sensorDate", DateType()),
    StructField("sensorReadings", ArrayType(
        StructType([
            StructField("sensorChannel", IntegerType()),
            StructField("sensorReading", DoubleType()),
            StructField("datetime", TimestampType())
        ])
    ))
])

sensor_df = spark.read\
    .format("json")\
    .schema(sensor_schema)\
    .option("multiline", True)\
    .option("path", "C:/Users/chait/OneDrive/Desktop/datasets/Prac datasets/sensor.json")\
    .load()

sensor_df_1 = sensor_df.select("sensorName", "sensorDate",
                 F.explode("sensorReadings").alias("sensorReadings"))\
    .select("sensorName", "sensorDate", "sensorReadings.*")


sensor_df_1.show(truncate=False)































